# -*- coding: utf-8 -*-


Automatically generated by Colaboratory.

"""

!pip install pyreadstat

#import library
import pickle
import pandas
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, log_loss
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from xgboost import XGBClassifier
import xgboost
import math
from sklearn import metrics
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Perceptron

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV, KFold

my_data_new=pd.read_spss('New 74M.sav')
my_data_new

scaler = RobustScaler()
features = [[ 'Age',
              'TEdProgramTopicsA1',
 'TEdProgramTopicsA5',
 'TEdProgramTopicsA18',
 'LongestPageNo',
 'OnlineActivities1',
 'OnlineActivities2',
 'OnlineActivities3',
 'OnlineActivities4',
 'OnlineActivities5',
 'OnlineActivities6',
 'ReadingBooks',
 'ReadingNews',
 'SkillsImportance',
 'SituationOccur',
 'TeachingTech',
 'OtherBelief',
 'JobFeeling',
 'Strategy']]
for feature in features:
  my_data_new[feature] = scaler.fit_transform(my_data_new[feature])
my_data_new

my_data_new['Outcome'].value_counts()

X = my_data_new.drop('Outcome',axis=1).values
X

y = my_data_new['Outcome'].values
y

#frequency for np array
unique, counts = np.unique(y, return_counts=True)
print(np.unique(y, return_counts=True))

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42, stratify=y)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

# Cross-validation for LogisticRegression -- Ask about this!
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import loguniform

# define model
model = LogisticRegression()

# define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=10)

# define search space
space = dict()
space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']
space['penalty'] = ['l1', 'l2']
space['C'] = loguniform(0.01, 10)

# define search
search = RandomizedSearchCV(model, space, n_iter=500, scoring='accuracy', n_jobs=1, cv=cv, random_state=10)

# execute search
result = search.fit(X_train, y_train)
# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

# define model
# my own model, outperforming the CV ones!
model_logistic = LogisticRegression(random_state=1, solver='newton-cg', penalty='l2', C=0.05)
log_reg = model_logistic.fit(X_train, y_train)
y_pred = log_reg.predict(X_test)
print(accuracy_score(y_test, y_pred))

#training accuracy
train_accuracy = model_logistic.score(X_train, y_train)
print(train_accuracy)

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)

#import classification_report
from sklearn.metrics import classification_report

print(classification_report(y_test,y_pred))

import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib import pyplot

# get importance
importance = model_logistic.coef_[0]
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
plt.bar([x for x in range(len(importance))], importance)
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
 
# defining parameter range
param_grid = {'C': [1, 10, 100],
              'gamma': [0.1, 0.01, 0.001],
              'kernel': ['rbf', 'linear', 'poly']}
 
grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
 
# fitting the model for grid search
grid.fit(X_train, y_train)

# print best parameter after tuning
print(grid.best_params_)
 
# print how our model looks after hyper-parameter tuning
print(grid.best_estimator_)

print('Best Score: %s' % result.best_score_)

#define model svm
#my own model, outperforming the CV ones!
from sklearn.svm import SVC

model_svm = SVC(random_state=1, kernel='rbf', C=1000, gamma=0.0001, probability=True)
svm = model_svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)
print(accuracy_score(y_test, y_pred))

#training accuracy
train_accuracy = model_svm.score(X_train, y_train)
print(train_accuracy)

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)

#import classification_report
from sklearn.metrics import classification_report

print(classification_report(y_test,y_pred))

# generate a no skill prediction (majority class)
from sklearn.metrics import roc_curve

ns_probs = [0 for _ in range(len(y_test))]
# predict probabilities
lr_probs_logistic = model_logistic.predict_proba(X_test)
lr_probs_svm = model_svm.predict_proba(X_test)
# keep probabilities for the positive outcome only
lr_probs_logistic = lr_probs_logistic[:, 1]
lr_probs_svm = lr_probs_svm[:, 1]
# calculate scores
ns_auc = roc_auc_score(y_test, ns_probs)
lr_auc_logistic = roc_auc_score(y_test, lr_probs_logistic)
lr_auc_svm = roc_auc_score(y_test, lr_probs_svm)

# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Logistic: ROC AUC=%.3f' % (lr_auc_logistic))
print('SVM: ROC AUC=%.3f' % (lr_auc_svm))
# calculate roc curves
ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)
lr_fpr_logistic, lr_tpr_logistic, _ = roc_curve(y_test, lr_probs_logistic)
lr_fpr_svm, lr_tpr_svm, _ = roc_curve(y_test, lr_probs_svm)
# plot the roc curve for the model
pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
pyplot.plot(lr_fpr_logistic, lr_tpr_logistic, marker='.', label='Logistic')
pyplot.plot(lr_fpr_svm, lr_tpr_svm, marker='.', label='SVM')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()
